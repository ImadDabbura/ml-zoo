{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Feature Preprocessing & Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Feature preprocessing and generation depends on the type model we are using. For example, tree-based models require/behave differently than non-tree-based models (such as linear models, NN, and KNN) so we may change the preprocessing/generation of features given the type of the model we are working with.For example, we don't use one-hot encoding with decision trees but it is required with linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Feature Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Scaling:\n",
    "    - Tree-based models are not affected by the scale of the features because the decision tree uses rank of the feature to find the best split and doesn't care about the scale of the features.\n",
    "    - Non-tree-based models suffer from features that have different scaling especially models that use gradient descent and distance metrics. Also, regularization proportionally affected by scaling.\n",
    "\n",
    "Therefore, scaling is required for non-tree-based models.\n",
    "\n",
    "Note that there are many ways to scale features and they have different effects on the performance of the model.\n",
    "- Min-Max scaler = $\\frac{(X - min(X))}{(max(X) - min(X)}$. This will make the feature have the values [0,1]. This method; however, does not change the distribution of the values.\n",
    "- Standardization = $\\frac{(X - mean(X)}{std(X)}$. This will result in change in the distribution of the data to have mean = 0 and standard deviation = 1.\n",
    "\n",
    "2. Outliers:\n",
    "    - Outliers can be in features or target values. Outliers affect mainly non-tree-based models especially NN and linear models. It either changes the coefficient of that feature or shift the line up/down to get closer to the outliers.\n",
    "    - We can solve the issue of outliers by clipping the values of features to be either between 1th and 99th percentile or 5th and 95th percentile. We can remove rows that have huge values to get stable predictions.\n",
    "    - `Rank` the feature can help us solve for outliers and bringing the outliers closer to other values and make the values have equal distance from each other. It first sorts the values and then replace each value by its rank. This is crucial for non-tree-based models especially if we don't have time to work on outliers. We can use scipy.rankdata.\n",
    "    - Note that outliers affect the scaling of features.\n",
    "\n",
    "3. Transformation:\n",
    "    - Such as log(1 + x) or sqrt(x + 2/3).\n",
    "    - This will make outliers close to the average value and values close to zero more distinguishable. NN get better results with such transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It helps with all types of models even with tree-based models such as GBDT since it finds difficulty with multiplications and devisions. It requires prior knowledge and also EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have two kinds of categorical features: Ordinal and Nominal.\n",
    "1. Ordinal:\n",
    "    - There is a meaning in the order of the categories such as small < big.\n",
    "2. Nominal:\n",
    "    - There is no order in the categories such as colors.\n",
    "3. Methods:\n",
    "    - LabelEncoding and FrequencyEncoding are helpful for tree-based models.\n",
    "    - OneHotEncoding is often used for non-tree-based models.\n",
    "    - Interactions of categorical features can help linear models and KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Datetime Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Periodicity:\n",
    "    - Day number in week, month, year, etc.\n",
    "    - It is helpful to figure out patterns in the data such as spikes on given days.\n",
    "2. Time since:\n",
    "    - Last holiday, event, etc.\n",
    "    - Row-independent moments such as days since something happened.\n",
    "    - Row-dependent moments such as days until next holiday or days past after last holiday.\n",
    "3. Difference between dates:\n",
    "    - Take the difference between dates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Interesting places in train/test data.\n",
    "- Center of clusters.\n",
    "- Aggregated statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Missing values can be: empty string, NaNs, -1, 99, etc. Sometimes we can tell if the number is missing using histograms. Usually the peak that looks an outliers are missing values. \n",
    "\n",
    "Approaches:\n",
    "1. Replace with something like -999, -1, etc. It hurts the performance of NN and linear models.\n",
    "2. Mean/Median imputation. It is mainly beneficial for NN and linear models.\n",
    "3. Reconstruct value using IterativeImputer or KNNImputer.\n",
    "\n",
    "Usually missing values have info behind them so it is recommended to use an indicator that check if a row has missing value. XGBoost and CatBoost can handle missing values better than the approaches discussed earlier. Note that sometimes we may replace outliers with missing values. Try to avoid filling nans before feature generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some of the most used preprocessing on text are: lowercase, stemming, lemmatization, and stopwords (don't contain important information such as 'the'). We can decide on the minimum_freq, max_freq, and max_features when using feature extraction.\n",
    "1. Bag of words:\n",
    "    - Assumes words independent and create one column for each feature. We can use something like CountVectorizer, TfidfCountVectorizer to get such matrices.\n",
    "    - Since non-tree-based models depend on the scaling of features, we can do the following postprocessing after CountVectorizer:\n",
    "        - Normalize the rows to have sum of 1 by dividing the frequency by total frequency in the document (document-wise normalization).\n",
    "        - Normalize the columns (words) so that less frequent words have more importance than frequent words (column-wise normalization).\n",
    "        - We can achieve the above two normalization using TFIDF.\n",
    "    - We can also use n-grams using either words or chars. ngram_range and analyzer help us achieve that. N-grams allow us to discover relationships between words/chars using their local context.\n",
    "    - Pros: Meaning of each value is known.\n",
    "    - Cons: Very large vectors.\n",
    "  \n",
    "2. Embeddings:\n",
    "    - Convert documents into dense vectors such that similar words are close in the new space.\n",
    "    - Some examples are word2vec and GloVe, Doc2vec. We can use the pretrained models.\n",
    "    - We can either take the average/sum of the words vectors or use Doc2vec.\n",
    "    - Pros: Dense vectors, and similar words have similar embeddings.\n",
    "    - Cons: Meaning of values will be mostly lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use either pre-trained models or train one from scratch. For most cases, using pre-trained models will be good enough. We can also fine-tune the pre-trained models to get a boost in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Advantages of EDA:\n",
    "- Better understand the data.\n",
    "- Build an intuition about the data.\n",
    "- Generate hypothesis.\n",
    "- Find insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Steps of EDA:\n",
    "1. Get domain knowledge - this means:\n",
    "    - Reading about the topic/problem we are trying to solve.\n",
    "    - Understand the meaning of the columns and what they measure.\n",
    "2. Check if the data is intuitive - this means:\n",
    "    - Checking if the values in each column are intuitive.\n",
    "3. Understand how the data was generated - it is crucial to understand the generation process to set up a proper validation scheme. For example, if the training data was generated by different algorithm than the test data, this will make it hard to use portion of the training data as a validation set because training and test have two different distributions. Therefore, in the worst cases, at least the validation and test sets have to come from the same distribution so that the scores on validation set would be a good proxy for the scores on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Histograms: Good to see the distribution of the features. Vary the number of the bins/bin size to get clear picture. Otherwise, histograms can deceive sometimes. It is recommended to use Empirical Distributions to get clear picture about the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Plot Index Vs Value: This will show us if there are both horizontal/vertical patterns, i.e. if the data is properly shuffled (no vertical patterns) and if there are a lot of repeated values (horizontal patterns). To make it even more helpful, we can color the dots by the target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`plt.plot(x, '.')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Scatter plot: It is helpful to see relationships between two features (we can also color it by the target values). We can also see if the train/test have overlapping values. If the test set have different values than train set, that is problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We can plot the correlation matrix.\n",
    "- We can do some kind of clustering and see which group of features are similar. We can add features such as aggregate statistics per group and use them as new features.\n",
    "- We can also test and see how many different distinct combinations there are between features as well as how many times a feature is greater than the other one.\n",
    "- We can compute the mean of each feature and use a scatter plot where the y-axis would have the means and the x-axis would have the feature names sorted by their mean values. We can sometimes see groups of features that have similar means and generate features accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "## Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- It is better to remove constant features, df.nunique(axis=1) == 1.\n",
    "- Drop duplicated columns, df.T.drop_duplicates(). If we have categorical features, we can use pandas.factorize that label encodes categories by order they appear so that it allows us to see the duplicates. df[feat].factorize() then df.T.drop_duplicates().\n",
    "- Check if there are common rows in the train and test sets. Remove them in case there are any.\n",
    "- Check if the data is shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole idea behind validation set is to have dataset that best approximate the model score when tested on unseen data. Therefore, this will help us avoid overfitting. Also, the validation data should have the same distribution as the test set. In some cases, training data may have different distribution than the test set.\n",
    "> Overfitting means that the validation error starts increasing (change direction from decreasing) because it starts capturing noise and patterns that are specific to the training data and are not present in the test data. Overfitting doesn't mean that training error is less than validation error because almost always training error is less than validation error.\n",
    "> Underfitting means the model couldn't capture the underlying relationship in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation strategies (Use with *Stratification*):\n",
    "- Holdout: Be careful from having duplicate rows between training and validation sets.\n",
    "    - `sklearn.model_selection.train_test_split`\n",
    "- K-fold Cross Validation.\n",
    "    - `sklearn.model_selection.KFold`\n",
    "- LeaveOneOut.\n",
    "    - `sklearn.model_selection.LeaveOneOut`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data splitting strategies:\n",
    "- Make train/valid split as similar as possible to train/test splits.\n",
    "- For time-dependent data, validation should be in the future.\n",
    "- Random (row-wise). The underlying assumption is that the rows are independent. Therefore, if there are some kind of dependencies between the rows, we can design new features that exploit this and boost our model.\n",
    "- Time-wise. We can create features such as average number of same day last week/month/year.\n",
    "- Ids. If the training set has completely different set of ids than the test set, then we have to do the same thing when we split the training data into train/valid data. Also, probably creating new features such as aggregate stats per id would not help too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues with validations:\n",
    "- Too diverse and inconsistent data.\n",
    "- Too little data.\n",
    "\n",
    "To solve for this issue, use CV and try to mimic train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Make sure there is not information from the future such as weather or user history.\n",
    "- Check if metadata includes some info about the future.\n",
    "- Ids are supposed to be generated by the system and should not be useful; however, sometimes they contain information that improves the model's performance.\n",
    "- Row order can be included as a feature and can contain information about the target because rows next to each other have same target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Good models don't necessarily have higher results for all metrics. At the beginning of the training, all metrics tend to improve. As the training continues, the metric we optimize for keeps improving while other metrics tend to usually deteriorate. Otherwise, we get suboptimal results if all the metrics for a given model have superior results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mean Squared Error (MSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$MSE = \\frac{1}{N}\\sum_{i=1}^N(y_i - \\hat{y_i})^2$$\n",
    "It measures average squared error for the dataset. It is the default error used in ML. MSE curve is hyper-parabola and if we want to use a constant as a baseline that minimize MSE (minimum point), this would be the *Mean* of $y$ ($\\bar{y})$. It puts bigger weights on higher values which makes it very sensitive to outliers. In other words, it tries to close the gap with the high values at the expense of the small target values; i.e. close the gap with the peak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Root Mean Squared Error (RMSE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N(y_i - \\hat{y_i})^2} = \\sqrt{MSE}$$\n",
    "The main advantage of using RMSE is that it has the same unit as the target variable so it would be easier to explain it; unlike MSE that is square that unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Every minimizer of MSE is a minimizer of RMSE and vise versa because square/square root are monotonic non-decreasing functions. In practice, people usually optimize for MSE but report the RMSE when done because MSE is easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The gradients of RMSE is different than RMSE because it depends on MSE (dynamic); therefore, it is different for gradient-based algorithms. We may need to change learning rates to account for that.\n",
    "$$\\frac{\\partial{RMSE}}{\\partial{y_i}} = \\frac{1}{2\\sqrt{MSE}}\\frac{\\partial{MSE}}{\\partial{y_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use RMSE or MSE if:\n",
    "- Can't afford large errors.\n",
    "- There are few large values but not outliers that we care about.\n",
    "- Errors are not analogous --> 4 is more than twice 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$MAE = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y_i}|$$\n",
    "The main advantage of this metric is that it treats all errors the same; i.e. doesn't penalizes large errors which make it less sensitive to outliers compared to MSE. This means that it would focus less on the high values. The best constant that minimizes MAE is the *Median*.\n",
    "\n",
    "Comparing MAE to MSE: error of 4 is twice the error of 2 with MAE; however, it is 4 times the error of 2 with MSE.\n",
    "\n",
    "MAE is not differentiable at 0. Also, second derivative is 0 everywhere.\n",
    "\n",
    "Use MAE if:\n",
    "- There are outliers in the data.\n",
    "- We have analogous errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mean Square Percentage Error (MSPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$MSPE = \\frac{100\\%}{N}\\sum_{i=1}^N(\\frac{y_i - \\hat{y_i}}{y_i})^2$$\n",
    "MSE and MAE care about absolute error and not relative error. For example, they treat 10 - 9 the same way as 100 - 99. However, the relative error is completely different. In the first case, it is 10% and in the second case it is 1%. \n",
    "\n",
    "It is the weighted version of MSE. The best constant for MSPE is weighted mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mean Absolute Percentage Error (MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$MAPE = \\frac{100\\%}{N}\\sum_{i=1}^N|\\frac{y_i - \\hat{y_i}}{y_i}|$$\n",
    "Weighted version of MAE. The best constant is weighted median. \n",
    "- All % errors are analogously weighted and it is not sensitive to outliers.\n",
    "- It can be misleading. You may have a model with low MAPE but missing very high value samples.\n",
    "- Don't use this metric if the range of values for the target is very large (high standard deviation), has a lot of zero, or there are unexpected spikes of ups downs because this would explode MAPE. The remedy for these issues is to add a constant to the target. Make sure that the constant is not too high (damage the model because it can't see variations in the target value) or too small (results in high MAPE). Therefore, treat as a hyper-parameter.\n",
    "\n",
    "Use MAPE if:\n",
    "- The target value doesn't have a big range of values and standard deviation remains small.\n",
    "- Target values are positive and far away from zero.\n",
    "- No unexpected spikes ups and downs.\n",
    "- Easily explain the results to the business stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "### Root Mean Squared Logarithmic Error (RMSLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "RMSE in the log space. The way to use this metric is to take the logarithm of the target value (adding small constant such 1 in case we have zeros). Using log transformation helps bring large values closer to the majority of the values as well as alleviates the small values from zero.\n",
    "\n",
    "Pros:\n",
    "- Not sensitive to outliers.\n",
    "- Put more weights on bigger errors.\n",
    "- Penalizes underprediction more than over prediction. This is very helpful if overprediction can be tolerated but underprediction can lead to problems/missing opportunities.\n",
    "\n",
    "Cons:\n",
    "- Don't work with negative values.\n",
    "- Can become insensitive to fluctuations in the target due to heavy penalization of the higher values.\n",
    "\n",
    "It is considered better than MAPE/MSPE because it is less biased towards small values and it also works with relative errors.\n",
    "\n",
    "Use RMSLE if:\n",
    "- Want relative errors.\n",
    "- Have positive target values.\n",
    "- Have outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### R-squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$R^2 = 1 - \\frac{MSE}{\\frac{1}{N}\\sum_i^N(y_i - \\bar{y_i})^2}$$\n",
    "The main advantage of $R^2$ over MSE/RMSE is that it tells u how good is the model with values range between [0, 1]. Therefore, it shows us how better is the model from the baseline model that always predict the mean of the target. The baseline has an $R^2 = 0$ when we predict the mean ($\\bar{y}$). If we optimize for $R^2$, it is the same thing as optimizing for MSE since the other components of $R^2$ are constants and don't affect optimization.\n",
    "\n",
    "Use $R^2$ to rank models.\n",
    "\n",
    "When MSE is the optimizer, $R^2$ is maximized; however, when MSE is not the optimizer, pearson correlation coefficient is used to compute the linear correlation between the target and the prediction of the model. But two models that have same $R^2$ may have completely different errors.\n",
    "However, it has some drawbacks:\n",
    "- It doesn't give us the error\n",
    "- It can be low for good models and high for bad models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 100, 100)\n",
    "y_hat = y + np.random.randint(0, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_log = np.log(y)\n",
    "y_hat_log = np.log(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09795937239543882"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y_log - y_hat_log) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5101598447800129"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log1p(600) - np.log1p(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3361867670217862"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log1p(1400) - np.log1p(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.692647555268513"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((np.log1p(2000) - np.log1p(1000)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.398594934535208, 7.244941546337007)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log1p(600), np.log1p(1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0486609783905383"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log1p(1400) / np.log1p(1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Soft labels: class probabilities.\n",
    "- Hard label: class predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$Accuracy = \\frac{1}{N}\\sum_{i=1}{N}[\\hat{y_i} = y_i]$$\n",
    "It measures how frequently class predictions are correct. The best constant is to predict the most frequent class.\n",
    "\n",
    "- It is misleading when the dataset is imbalanced. For example, if a dataset has 99% of the data belong to one class, then the baseline accuracy is 99%.\n",
    "- It is very hard to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- If the task is binary, pick any metric and optimize the threshold.\n",
    "- If the task is multiclass, sort the models by their accuracies not by the metric used for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LogLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Binary:\n",
    "$$Negative Log Loss = -\\frac{1}{N}\\sum_{i=1}^{N}(y_ilog(\\hat{y_i}) - (1 - y_i)log(1 - \\hat{y}_i))$$\n",
    "Multiclass:\n",
    "$$Negative Log Loss = -\\frac{1}{N}\\sum_{i=1}^{N}y_{ik}log(\\hat{y}_{ik})$$\n",
    "- It penalizes very wrong answers. The more you deviate from the actual class, the more you get penalizes.\n",
    "- The best constant is the frequency of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Area Under ROC Curve (AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This metric doesn't fix the threshold but computes everything under all possible thresholds. It only works for binary tasks. It also depends only on the order of predictions not their absolute values.\n",
    "- The range of values [0, 1].\n",
    "- The baseline is 0.5 which is randomly predicting the class of the new object.\n",
    "- It measures the ability to differentiate between positive and negative classes (discriminate between classes). For example, if we have two instances A & B, when the model says A is more likely than B, how often is A actually True and B False. The higher the AUC, the more separable the classes are.\n",
    "- It is usually used in an imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cohen's Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$Cohen's Kappa = 1 - \\frac{1 - accuracy}{1 - baseline\\ accuracy} = \\frac{observed\\ accuracy - expected\\ accuracy}{1 - expected\\ accuracy}$$\n",
    "The Kappa statistic (or value) is a metric that compares an **Observed Accuracy** (classifier got right; i,e. TP and TN) with an Expected Accuracy (random chance). The kappa statistic is used not only to evaluate a single classifier, but also to evaluate classifiers amongst themselves (or one rater against another rater).\n",
    "$$Observed\\ accuracy = \\frac{TP + TN}{N}$$\n",
    "$$Expected\\ accuracy = \\frac{\\frac{((TN + FP) * (TN + FN))}{N} + \\frac{((TP + FN) * (TP + FP))}{N}}{N}$$\n",
    "To get the baseline accuracy, we multiply the relative frequency of each class from the dataset with its predictions proportion.\n",
    "- The range of values [-1, 1].\n",
    "- If the value is zero, then the classifier has the score as random classifier. Anything above zero is better. The bigger the score, the more agreement. Therefore, the perfect score is 1.\n",
    "\n",
    "We can use weight matrix to get weighted Cohen's Kappa. \n",
    "If we have a problem such as the severity of the disease, this is not really regression so we can treat it as classification and use weight matrix to assign different weight for different errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### F-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The traditional F-measure or balanced F-score is the harmonic mean of precision and recall:\n",
    "\n",
    "$$F_1 = \\left(\\frac{2}{\\mathrm{recall}^{-1} + \\mathrm{precision}^{-1}}\\right) = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$$\n",
    "\n",
    "The general formula for positive real β, where β is chosen such that recall is considered β times as important as precision, is:\n",
    "$$F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}$$\n",
    "\n",
    "The formula in terms of Type I and type II errors:\n",
    "\n",
    "$$F_\\beta = \\frac {(1 + \\beta^2) \\cdot \\mathrm{true\\ positive} }{(1 + \\beta^2) \\cdot \\mathrm{true\\ positive} + \\beta^2 \\cdot \\mathrm{false\\ negative} + \\mathrm{false\\ positive}}$$\n",
    "\n",
    "Two commonly used values for β are those corresponding to the $F_{2}$ measure, which weighs recall higher than precision (by placing more emphasis on false negatives), and the $F_{0.5}$ measure, which weighs recall lower than precision (by attenuating the influence of false negatives).\n",
    "\n",
    "The F-measure was derived so that $F_\\beta$ \"measures the effectiveness of retrieval with respect to a user who attaches β times as much importance to recall as precision\".\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "- Minimize False Positives -> Use Precision or set $\\beta < 1$\n",
    "- Minimize False Negatives -> Use Recall or set $\\beta > 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Probability Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If the model doesn't optimize the LogLoss directly, we can calibrate the probabilities on the predictions of the classifier. This will help us with uncertainties of each each predictions and understand the model better. Even if models give us probabilities such as `RandomForest`, it does not mean they are good probabilities because in extreme case where the node is 100% pure, the probability would be 100% even though it has some uncertainty.\n",
    "\n",
    "Definition: Predicted probabilities that match the expected distribution of probabilities for each class.\n",
    "\n",
    "Most Non-linear ML models don't make probabilistic predictions and instead use approximations -> The predicted probabilities don't match the expected distribution of the observed data -> Needs calibration.\n",
    "\n",
    ">\"we desire that the estimated class probabilities are reflective of the true underlying probability of the sample. That is, the predicted class probability (or probability-like value) needs to be well-calibrated. To be well-calibrated, the probabilities must effectively reflect the true likelihood of the event of interest.\"\n",
    "\n",
    "For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Reliability Diagrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    ">Reliability diagrams are common aids for illustrating the properties of probabilistic forecast systems. They consist of a plot of the observed relative frequency against the predicted probability, providing a quick visual intercomparison when tuning probabilistic forecast systems, as well as documenting the performance of the final product.\n",
    "\n",
    "Specifically, the predicted probabilities are divided up into a fixed number of buckets along the x-axis (sort the predictions first and then divide them into buckets). The number of events (class=1) are then counted for each bin (e.g. the relative observed frequency). Finally, the counts are normalized. The results are then plotted as a line plot (we use the center of the bins).\n",
    "\n",
    ">Reliability diagrams provide a diagnostic to check whether the forecast value Xi is reliable. Roughly speaking, a probability forecast is reliable if the event actually happens with an observed relative frequency consistent with the forecast value.\n",
    "\n",
    "The better calibrated or more reliable a forecast, the closer the points will appear along the main diagonal from the bottom left to the top right of the plot.\n",
    "\n",
    "The position of the points or the curve relative to the diagonal can help to interpret the probabilities; for example:\n",
    "- **Below the diagonal**: The model has over-forecast; the probabilities are too large.\n",
    "- **Above the diagonal**: The model has under-forecast; the probabilities are too small.\n",
    "\n",
    "Models such as logistic regression have well calibrated probabilities; however, decision trees and NN don't produce calibrated probabilities.\n",
    "\n",
    ">Platt Scaling is most effective when the distortion in the predicted probabilities is sigmoid-shaped. Isotonic Regression is a more powerful calibration method that can correct any monotonic distortion. Unfortunately, this extra power comes at a price. A learning curve analysis shows that Isotonic Regression is more prone to overfitting, and thus performs worse than Platt Scaling, when data is scarce.\n",
    "\n",
    "**Note**: Better calibrated probabilities may or may not lead to better class-based or probability-based predictions. It really depends on the specific metric used to evaluate predictions. In fact, some empirical results suggest that the algorithms that can benefit the most from calibrating predicted probabilities include SVMs, bagged decision trees, and random forests.\n",
    "\n",
    ">\"after calibration the best methods are boosted trees, random forests and SVMs.\"\n",
    "\n",
    "**Diagnosis using Scikit-learn**:\n",
    "\n",
    ">This can be implemented by first calculating the calibration_curve() function. This function takes the true class values for a dataset and the predicted probabilities for the main class (class=1). The function returns the true probabilities for each bin and the predicted probabilities for each bin. The number of bins can be specified via the n_bins argument and default to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calibrate Probabilities Scikit-learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    ">This can be implemented by first calculating the calibration_curve() function. This function takes the true class values for a dataset and the predicted probabilities for the main class (class=1). The function returns the true probabilities for each bin and the predicted probabilities for each bin. The number of bins can be specified via the n_bins argument and default to 5.\n",
    "\n",
    "A classifier can be calibrated in scikit-learn using the CalibratedClassifierCV class. There are two ways to use this class: prefit and cross-validation. \n",
    "- You can fit a model on a training dataset and calibrate this prefit model using a hold out validation dataset.\n",
    "- Alternately, the CalibratedClassifierCV can fit multiple copies of the model using k-fold cross-validation and calibrate the probabilities predicted by these models using the hold out set. Predictions are made using each of the trained models.\n",
    "- The CalibratedClassifierCV class supports two types of probability calibration; specifically, the parametric ‘sigmoid‘ method (Platt’s method) and the nonparametric ‘isotonic‘ method which can be specified via the ‘method‘ argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Example with SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SVM reliability diagrams with uncalibrated and calibrated probabilities\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# predict uncalibrated probabilities\n",
    "def uncalibrated(trainX, testX, trainy):\n",
    "\t# fit a model\n",
    "\tmodel = SVC(gamma='auto')\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# predict probabilities\n",
    "\treturn model.decision_function(testX)\n",
    "\n",
    "# predict calibrated probabilities\n",
    "def calibrated(trainX, testX, trainy):\n",
    "\t# define model\n",
    "\tmodel = SVC(gamma='auto')\n",
    "\t# define and fit calibration model\n",
    "\tcalibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
    "\tcalibrated.fit(trainX, trainy)\n",
    "\t# predict probabilities\n",
    "\treturn calibrated.predict_proba(testX)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate 2 class dataset\n",
    "X, y = make_classification(n_samples=1000, n_classes=2, weights=[1,1], random_state=1)\n",
    "\n",
    "# split into train/test sets\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "# uncalibrated predictions\n",
    "yhat_uncalibrated = uncalibrated(trainX, testX, trainy)\n",
    "\n",
    "# calibrated predictions\n",
    "yhat_calibrated = calibrated(trainX, testX, trainy)\n",
    "\n",
    "# reliability diagrams\n",
    "fop_uncalibrated, mpv_uncalibrated = calibration_curve(testy, yhat_uncalibrated, n_bins=10, normalize=True)\n",
    "fop_calibrated, mpv_calibrated = calibration_curve(testy, yhat_calibrated, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot perfectly calibrated\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--', color='black')\n",
    "\n",
    "# plot model reliabilities\n",
    "pyplot.plot(mpv_uncalibrated, fop_uncalibrated, marker='.')\n",
    "pyplot.plot(mpv_calibrated, fop_calibrated, marker='.')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Mean Encoding (aka Target Encoding/Likelihood Encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In its simplest form, we would have a feature that computes the mean of target of each category. \n",
    "- This is better than label encoding because it helps separate classes.\n",
    "- Gradient Boosting Trees struggle to handle high cardinality random variables. So adding mean encoding will help reduce the depth of the tree and improve performance compared to the simple label encoding which gives random order to labels with no correlation with the target.\n",
    "- It is especially helpful if the relationship between the feature and target is highly nonlinear.\n",
    "- Be careful from overfitting because when adding features that use the target, we most likely overfit unless we do careful validation/CV.\n",
    "- Ways to use target variable:\n",
    "    - Likelihood = mean(target)\n",
    "    - Weight of Evidence = ln(count(positive) / count(negative)) * 100\n",
    "    - Count = sum(target)\n",
    "    - Diff = count(positive) - count(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For each data point in a given fold, we estimate the encoding using the data points in the other folds.\n",
    "- It is robust and intuitive.\n",
    "- Usually 4-5 folds are enough.\n",
    "- If we have a category that is not present in other folds, use global mean of the target (prior).\n",
    "- This method is stable for categories that are present is descent number.\n",
    "- We can use smoothing as a form of regularization. Rare categories can be punished using smoothing.\n",
    "- We can also add noise to the data. As noise increases -> more regularization and can destroy encoding. So noising level is a hyperparameter.\n",
    "- We can also use expanding mean by predicting each data point encoding using $n - 1$ data points. This method is easy and has the least target leakage (built-in in CatBoost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "## Regression & Multicass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We can use many statistics such as percentiles, std, and bins distribution.\n",
    "- With time series, we can use rolling statistics of target variable such as mean or sum.\n",
    "- Binning and selecting interactions of numerical features:\n",
    "    - We can fit a model such as XGBoost and analyze the model structure.\n",
    "        - If a feature has a lot of split points, that is a good indication that we should use mean encodings on that feature. Also, those exact split points can be used to bin the features. \n",
    "        - To get good feature interactions, we can:\n",
    "            - Loop over all the trees and calculate the frequency of feature interactions. The highest the frequency, the more useful the interactions. We can use this method with 1-ways, 3-ways, etc. interactions.\n",
    "\n",
    "**Advice:** If we have a dataset with a lot of categorical features, it is worth it to try to use both mean encodings and interaction features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "## Correct Validation Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Estimate encodings of X_train.\n",
    "- Map them to X_train and X_val.\n",
    "- Regularize on X_tr.\n",
    "- Validate the model on both X_train and X_val.\n",
    "- When done:\n",
    "    - Estimate encodings on combined X_train and X_val.\n",
    "    - Map them to whole train and X_test.\n",
    "    - Regularize on whole train.\n",
    "    - Fit on whole train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "hidden": true
   },
   "source": [
    "## Advantages/Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Advantages:\n",
    "    - Compact transformation of categorical features.\n",
    "    - Powerful basis for feature engineering.\n",
    "- Disadvantages:\n",
    "    - Need careful validation, there are a lot of ways to overfit.\n",
    "    - Significant improvements only on specific datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'X': ['a', 'b', 'a', 'b', 'c'], 'y': [0, 0, 1, 1, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.526894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.526894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.526894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.526894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X\n",
       "0  0.526894\n",
       "1  0.526894\n",
       "2  0.526894\n",
       "3  0.526894\n",
       "4  0.600000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce.TargetEncoder().fit_transform(df['X'], df['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Practical Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Define your goal before you participate in any competition:\n",
    "    - Learn more about an interesting problem\n",
    "    - Get acquainted with new software tools\n",
    "    - Get a medal\n",
    "        - If people in the top have huge number of submissions -> mostly are doing probing or facing difficulty between validation and public leader board score\n",
    "        - If people in the top have few submissions -> they discovered non-trivial way of solving the problem\n",
    "2. After you enter competition:\n",
    "    - Organize your ideas in some structure\n",
    "    - Select most important and promising ideas\n",
    "    - Try to understand the reasons why something does/doesn't work\n",
    "    - Everything is a hyperparameter. Try to sort all parameters by their importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Do basic preprocessing and convert csv/txt files into hdfs(pandas)/npy(numpy arrays) for much faster loading\n",
    "2. By default, data is stored in 64-bit arrays. Most of the times we can safely downcast to 32-bits that would save us a lot of memory\n",
    "3. Large datasets can be processed in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Extensive validation is not always needed. Even with a dataset that has 50k or 100k rows, a simple train/test split works. Switch to CV when needed only (when marginal improvements is critical)\n",
    "2. Start with fastest models such as LightGBM\n",
    "3. Below is a simple modeling process flow:\n",
    "<img src=\"materials/modeling-process-flow.png\" heigh=\"200\" width=\"400\">\n",
    "4. Switch to ensemble and stacking when you are satisfied with feature engineering\n",
    "5. Don't pay attention to code quality\n",
    "6. Save only important things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Initial Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Start with EDA\n",
    "1. Start with simple or even primitive solution\n",
    "3. Debug full pipeline\n",
    "4. Start with simple model that is robust and requires small to no hyperparameter tuning\n",
    "5. Check if the validation is stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## Best Practices from Software Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Use good variable names\n",
    "2. Keep your research reproducible\n",
    "    - Fix random seed\n",
    "    - Write down exactly how each feature was generated\n",
    "    - Use version control\n",
    "3. Move reusable code into separate functions. It would be better if you create a simple library that includes a lot of the code that are commonly used in many problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true
   },
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Read scientific papers about the problem to get more knowledge and ideas about how to solve the problem. This usually helps with understanding of the features and generate new features as well as optimizing ML related tasks\n",
    "2. Overfit the training data and then regularize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Competition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"materials/competition-pipeline.png\" height=\"100px\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make necessary preparations to understand the problem such as reading related materials and get more business understanding of the problem\n",
    "    - Type of problem\n",
    "    - How big is the data\n",
    "    - Hardware needed\n",
    "    - Software needed. Then create a virtual environment for the project\n",
    "    - What is the metric being tested on?\n",
    "    - Check if I have done similar project before and reuse what worked before trying anything new\n",
    "2. Understand the features and what we have available. Also what are the dynamics and the process of the data generation\n",
    "    - Plot histograms of variables and check if the feature is similar between train and test. In most cases, big discrepancies can cause problems to the model\n",
    "    - Plot features versus target variable and versus time\n",
    "    - Consider univariate predictability metrics such as R/IV/AUC\n",
    "    - Binning numerical features to check if there are some nonlinearities\n",
    "    - Correlation matrices (Spearman correlation)\n",
    "3. Define a good validation strategy\n",
    "    - Validation should be as close as possible to the test set\n",
    "    - If time is important -> Use time-based split\n",
    "    - If the test has different entities (such as customers) than the train, then validation data should have different entities so that it resembles the test data\n",
    "    - If it is completely random, just use random split\n",
    "4. Features Engineering <--> Modeling\n",
    "    - <img src=\"materials/feature-engineering.png\">\n",
    "    - <img src=\"materials/modeling.png\">\n",
    "5. Ensembling\n",
    "    - Averaging to multilayer stacking\n",
    "    - Helps to average few low-correlated predictions\n",
    "    - With stacking, we treat predictions as features and repeat the same modeling process in terms of feature engineering and feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The idea is to combine good models that are as uncorrelated as possible to get the best performance. The less correlated the models are, the less correlated the models' errors. This means averaging the models would lead to lower errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have three ways to construct ensembles using:\n",
    "- Simple average. Take the average of multiple models\n",
    "- Weighted average. Assign weights to different models and combine them\n",
    "- Conditional average. Pick different models for different values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "RandomForest is a form of bagging where we build models on different subsets of the data and then combine the models. Bagging doesn't overfit but after some point there would be no improvement. It also can be parallelized.\n",
    "\n",
    "There are two sources of errors in modeling:\n",
    "- Errors due to **Bias** (underfitting)\n",
    "- Errors due to **Variance** (overfitting)\n",
    "\n",
    "Parameters that control bagging:\n",
    "- Random seed, we can simply run the same model with different seeds and average predictions\n",
    "- Row Subsampling or bootstrapping\n",
    "- Column Subsampling\n",
    "- Shuffling\n",
    "- Model-specific parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Boosting** is a form of weighted averaging of models where each model is built sequentially by taking into account the performance of the previous model. Boosting models can't be parallelized and more estimators can overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Weight-based Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"materials/weight-based-boosting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We fit a model on all features and calculate the absolute error for all samples\n",
    "- We then give more weights on rows that are very wrong and keep iterating\n",
    "- `AdaBoost` is an example of weight-based boosting\n",
    "\n",
    "Boosting parameters:\n",
    "1. Learning rate (eta or shrinkage). It is the contribution of each model in the final prediction, i.e. it can be seen as weight of each model\n",
    "2. Number of estimators (models). There is usually an inverse relationship between learning rate and the number of estimators. As learning rate decreases, we should increase the number of estimators. The way to tune them is by first fixing the number of estimators and tune the learning rate, then multiply the number of estimators by *c* and divide the learning rate by *c* and see how it works\n",
    "3. Input model - any model that accepts weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Residual-based Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"materials/residual-based-boosting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is more powerful than the weight-based boosting.\n",
    "- We fit a model on all features and calculate the error for all samples\n",
    "- Then the resulted error would be the new target variable\n",
    "- We then fit another model on the new target variable and keep iterating\n",
    "- `XGBoost` is an example of residual-based boosting\n",
    "\n",
    "Boosting parameters:\n",
    "1. Learning rate (eta or shrinkage). The first model would have fully contribute and then from 2nd model the contribution would be $lr * \\hat{y}$. Therefore, it is movement in the direction of the error so that we avoid overfitting.\n",
    "2. Number of estimators (models). There is usually an inverse relationship between learning rate and the number of estimators. As learning rate decreases, we should increase the number of estimators. The way to tune them is by first fixing the number of estimators and tune the learning rate, then multiply the number of estimators by *c* and divide the learning rate by *c* and see how it works.\n",
    "- Row Subsampling\n",
    "- Column Subsampling\n",
    "- Input model - better with trees\n",
    "\n",
    "Implementations:\n",
    "1. XGBoost\n",
    "2. LightGBM - very fast and efficient\n",
    "3. Catboost - handles missing values and categorical features out of the box. Plus it has good starting hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Stacking** means making predictions of a number of models (base learners) on a hold-out dataset and then use a different model (meta learner) to train on these predictions.\n",
    "\n",
    "Things to be mindful of:\n",
    "1. Diversity is as important as performance of base learners\n",
    "2. Diversity may come from:\n",
    "    - Different algorithms\n",
    "    - Different input features, i.e. we may have the same model run on different features transformations\n",
    "    - No risk of overfitting but after N models, the performance will plateau\n",
    "    - The meta model is usually not that complex because the predictions are already correlated with the target variable. Therefore, the model may not need to a lot of operations to uncover hidden structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**StackNet** is a scalable meta modeling methodology that utilizes stacking to combine multiple models in a neural network architecture of multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. 1st level tips:\n",
    "    - Diversity based on algorithms \n",
    "        - 2-3 gradient boosted trees using maybe different implementation such as LightGBM, XGBoost, and CatBoost or try to have one with small depth, one with middle depth, and one with deep depth and tune the hyperparameters around it. This way you would have as much asymmetry as possible in terms of performance\n",
    "        - 2-3 NN with different architectures\n",
    "        - 1-2 ExtraTrees/RandomForest\n",
    "        - 1-2 linear models such as logistic/ridge/svm\n",
    "        - 1-2 KNN models. They may have bad performance but usually add value in meta modeling setup\n",
    "    - Diversity based on input data\n",
    "        - Categorical features: such as label encoding, onehot encoding, etc\n",
    "        - Numerical feature: such as binning, outliers, percentiles \n",
    "        - Interactions: such as multiplication/division, groupby, unsupervised (SVD/PCA/KMeans...)\n",
    "2. Subsequent level tips:\n",
    "    - Shallower/simpler algorithms        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do we tune hyperparameters:\n",
    "1. Select the most important hyperparameters. We can get this list using multiple online resources/documentation.\n",
    "2. Understand how exactly they influence the training\n",
    "3. Tune them either manually or automatically (such as hyperopt)\n",
    "\n",
    "Libraries for hyperparameter optimization:\n",
    "- Hyperopt\n",
    "- Scikit-optimize\n",
    "- Spearmint\n",
    "- GPyOpt\n",
    "- RoBO\n",
    "- SMAC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Hyperparameters with green color means the complexity of the model increases as their values increase.\n",
    "- Hyperparameters with red color means the complexity of the model decreases as their values increase.\n",
    "\n",
    "<img src=\"materials/GBDT.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. If increasing the depth of tree is not causing the model to overfit, this means there are better interactions that can be extracted from the data. `max_depth` of 7 is a good starting point. Be careful that the higher the depth the longer it takes to train.\n",
    "2. `subsample` is the fraction of objects used when fitting a tree, [0, 1]. With lesser objects, the model takes more time to capture the underlying structure but will be more generalizable. Therefore, it can be seen as a regularizer.\n",
    "3. `colsample_bytree` and `colsample_bylevel` is the fraction of features used when fitting a tree. Same consequence as `subsample`.\n",
    "4. `min_child_weight` is one of the most important one to tune. The higher the value the more constrained the model is.\n",
    "5. `eta` and `num_round`. The higher the learning rate the faster the model fit the training dataset but most likely would overfit. The best strategy is to freeze the learning rate, say 0.1, and tune the number of trees using early stopping. Then multiply the number of trees by $\\alpha$ and divide the learning rate by $\\alpha$. This usually improves the scores.\n",
    "6. `seed` is usually fixed to be able to reproduce the model. However, if same model with different random seeds have totally different validation scores, then that is a sign that either the validation scheme is not good or the results are random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## RandomForest/ExtraTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. `num_estimators` the higher the better without the risk of overfitting because models are independent. Try first multiple values and plot the validation curve to select the best value since after some point the performance plateau.\n",
    "2. `max_features` is similar to `colsample` in GBDT.\n",
    "3. `max_depth` is usually higher than GBDT. 7 is a good starting point.\n",
    "4. `min_sample_leaf` is the same as `min_child_weight` in GBDT.\n",
    "5. `criterion` which can be *gini* or *entropy*. *gini* works better most of the times.\n",
    "\n",
    "The most important ones are the first two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tuning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is much better than Grid Search especially if some hyperparameter are not that important compared to other ones. We usually use random distributions for the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Bayesian Optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Also called sequential model based optimization SMBO):\n",
    "- Fit 'cheap' probabilistic function to black-box\n",
    "- We pick next point using exploration / exploitation\n",
    "- Implemented as acquisition function\n",
    "\n",
    "A surrogate is a function that approximates an objective function. The surrogate is useful because it takes little time to evaluate. So, for example, to search for a point that minimizes an objective function, simply evaluate the surrogate on thousands of points, and take the best value as an approximation to the minimizer of the objective function.\n",
    "\n",
    "Surrogate optimization is best suited to time-consuming objective functions. The objective function need not be smooth, but the algorithm works best when the objective function is continuous.\n",
    "\n",
    "Surrogate optimization attempts to find a global minimum of an objective function using few objective function evaluations. To do so, the algorithm tries to balance the optimization process between two goals: exploration and speed.\n",
    "\n",
    "Exploration to search for a global minimum.\n",
    "Speed to obtain a good solution in few objective function evaluations.\n",
    "The algorithm has been proven to converge to a global solution for continuous objective functions on bounded domains.\n",
    "\n",
    "In general, there is no useful stopping criterion that stops the solver when it is near a global solution. Typically, you set a stopping criterion of a number of function evaluations or an amount of time, and take the best solution found within this computational budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Multi-Fidelity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Approximate the function by similar cheaper function. The idea is to fit the model on a small subset of the data and pick the best hyperparameters. Usually values for best hyperparameters don't change much if at all when training on full dataset. We can also run the model for fewer number of iterations instead of waiting to finish training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given n configurations and budget B:\n",
    "- Pick η=2, η = 3 (wording follows 2)\n",
    "- Each iteration, keep best halve of configurations\n",
    "- After k=logη(n)+1, left with single configuration.\n",
    "- Initially allocate $\\frac{B}{kn}$ to each configuration, double each iteration\n",
    "\n",
    "For example, if we have configurations n = 81 and total budget B = 20000:\n",
    "- train 81 configurations with resources   41\n",
    "- train 27 configurations with resources  123\n",
    "- train  9 configurations with resources  370\n",
    "- train  3 configurations with resources 1111\n",
    "- train  1 configurations with resources 3333\n",
    "\n",
    "Therefore, resources total: 16638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true",
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Meta-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Learning from experience (other datasets). So basically every dataset is a sample and we learn from each dataset what is the best configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We can calculate statistics by groups of categories or one category such as minimum price by category and add those generated features as new features.\n",
    "- We can also use statistics applied to neighbors such as number of houses in 500m radius.\n",
    "- We can use matrix factorization techniques such as PCA/SVD/NMF. It is useful for tree-based models.\n",
    "- Interactions can be generated for both numerical and categorical features such as sum, mult, diff, division for numerical features and all possible combination of categorical features. Then use feature selection such as feature importance from RandomForest to select the best ones.\n",
    "    - We also use decision trees such as RandomForest or XGBoost to transform matrices into indices.\n",
    "   \n",
    "- tSNE: It is a manifold learning method that projects data into lower dimension while preserving the distance between points.\n",
    "    - The projections are highly dependable on hyperparameters mainly perplexities.\n",
    "    -  It is stochastic in nature, so it may provide different results for the same data/hyperparameters.\n",
    "    - It is slow if we have huge number of features. So we may need to use PCA then tSNE.\n",
    "    - It sometimes shows cluster that does not exist in original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**CatBoost** is stable and has good performance with default hyperparameters.\n",
    "\n",
    "Problems with gradient boosting that CatBoost tries to solve:\n",
    "1. Categorical features:\n",
    "    - Uses one-hot encoding for features with <= max_size categories\n",
    "    - Computes number of appearances for each category\n",
    "    - Statistics with label usage on a random permutation of the data such as mean encoding\n",
    "        - The way it is done is by using random permutation that calculates, for example, the average target value for object with the same categories that were before the current category\n",
    "        - Other useful statistics can be used too\n",
    "    - Statistics on feature combinations. This is usually done during each split where combinations are useful because the number of possible combinations grow exponentially with the number of categorical features\n",
    "    - Generates combinations of numerical and categorical features too\n",
    "    \n",
    "2. Hyperparameter tuning\n",
    "3. Prediction speed\n",
    "4. Overfitting. Traditional calculation of gradient of leaf value is by average the gradients of the objects that make up the node that the model was trained on. This will make it biased and more likely to overfit. With CatBoost, we average the gradients of all the past objects.\n",
    "5. Training speed\n",
    "\n",
    "CatBoost uses **Symmetric Decision Trees** where nodes on the same layer have the same split. This will help in terms of making the model less prune to overfitting and stable. Plus it is 30-60x faster than XGBoost.\n",
    "\n",
    "To speedup training, we can use `rsm` that specifies that fraction of features to look at on each split. With lower `rsm`, we need more iterations but overall speed will be faster.\n",
    "\n",
    "The support for missing values are good out of the box. It treats missing values in categorical features as its own categories and for numerical features it is replaced with values either higher or lower than the past objects and put into its own nodes.\n",
    "\n",
    "We can use early stopping to stop training after validation errors stop improving.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
